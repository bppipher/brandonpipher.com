---
title: Neural Network Image Classification
author: Brandon P. Pipher
date: '2020-04-14'
slug: []
categories:
  - Machine Learning
  - Image Classification
tags:
  - keras
  - R
  - tensorflow
subtitle: 'Utilizing keras in R on MNIST-style data sets'
summary: 'Utilizing keras in R on MNIST-style data sets'
authors: []
lastmod: '2020-04-14T14:09:00-04:00'
featured: no
image:
  caption: 'Cynarina (Cynarina lacrymalis)'
  focal_point: ''
  preview_only: no
projects: []
draft: false
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#keras-and-beyond">Keras and beyond</a></li>
<li><a href="#mnist">MNIST</a></li>
<li><a href="#fashion-mnist">Fashion MNIST</a></li>
<li><a href="#cifar-10">CIFAR-10</a></li>
</ul>
</div>

<div id="keras-and-beyond" class="section level1">
<h1>Keras and beyond</h1>
<p>Keras is a high-level neural networks API developed with a focus on enabling fast experimentation. Originally written in Python, the library <code>keras</code> is part of a series of R interfaces to TensorFlow.</p>
<p>TensorFlow is an open-source machine learning framework from Google which includes the Neural Network library of Keras.</p>
<p>The goal in this post is to do some low level examples on MNIST-like data sets and act as an introduction in applying neural network classifications techniques. For the sake of brevity this post is short on the details with the intention of being revisited.</p>
<p>Here’s a quick list of the packages we’ll be using:</p>
<pre class="r fold-show"><code>library(keras)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(imager)</code></pre>
</div>
<div id="mnist" class="section level1">
<h1>MNIST</h1>
<p>The <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> database is composed of handwritten digits and is a common “Hello, World!” data set for neural networks.</p>
<pre class="r fold-show"><code>mnist &lt;- dataset_mnist()</code></pre>
<p>Taking a look at an example of the images:</p>
<pre class="r fold-show"><code># Plotting helper; mnist is global var
mnistplot &lt;- function(picked){
  mnist$train$x[picked,,] %&gt;% 
  # Wrangle
  as_tibble() %&gt;% 
  rowid_to_column(&quot;Y&quot;) %&gt;% 
  pivot_longer(-Y,names_to = &quot;X&quot;, values_to = &quot;PIX&quot;) %&gt;% 
  mutate(X = as.numeric(gsub(&quot;V&quot;,&quot;&quot;,X))) %&gt;%
  mutate(Y = abs(Y - max(Y)+1)) %&gt;%
  # Graph
  ggplot(aes(X,Y, fill = PIX)) + 
  geom_tile() + 
  scale_fill_gradient(low=&quot;white&quot;, high=&quot;black&quot;, guide = FALSE) +
  ggtitle(paste(&quot;Handwritten number:&quot;,mnist$train$y[picked])) +
  theme_void()
}
# Grid graph
pickcount &lt;- 9
mnistex &lt;- lapply(1:pickcount, mnistplot)
mnistexplot &lt;- arrangeGrob(grobs = mnistex,
                           ncol = 3,
                           nrow = 3)
plot(mnistexplot)</code></pre>
<p><img src="/post/2020-04-14-image-classification-with-mnist-and-mnist-like-data/index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>So now that we’ve gotten an idea for what it is we’re trying to predict on let’s get started.</p>
<p>The first step is to normalize the data to help with training time.</p>
<pre class="r fold-show"><code>mnist_train = mnist$train
mnist_test  = mnist$test
# Normalization; Speeds training time
mnist_train$x &lt;- mnist_train$x/255
mnist_test$x  &lt;- mnist_test$x/255</code></pre>
<p>Now we’re ready to put together the actual model. I’ll be doing a sequential model as it’s the easiest to work with in Keras. It gets its name as we actually build the model layer by layer, sequentially.</p>
<pre class="r fold-show"><code>mnist_model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28,28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)</code></pre>
<p>Here we can look at a summary of the model we’ve put together detailing the parameters and layers.</p>
<pre class="r fold-show"><code>summary(mnist_model)</code></pre>
<pre><code>## Model: &quot;sequential&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## flatten (Flatten)                   (None, 784)                     0           
## ________________________________________________________________________________
## dense (Dense)                       (None, 128)                     100480      
## ________________________________________________________________________________
## dropout (Dropout)                   (None, 128)                     0           
## ________________________________________________________________________________
## dense_1 (Dense)                     (None, 10)                      1290        
## ================================================================================
## Total params: 101,770
## Trainable params: 101,770
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<p>From here we need to actually compile our model. When we compile the model is when we define what loss function will be optimized and what optimizer will be used to accomplish this.</p>
<pre class="r fold-show"><code>mnist_model %&gt;% 
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )</code></pre>
<p>Oddly enough, when building or compiling with Keras we’re actually modifying our model in place. This is a little bit of strange behavior in R that’s easy to forget.</p>
<p>Our next step is to <em>actually</em> fit the model to our data. Here we assign the cross validation ratio and the number of epochs and let it form a model to use for predictions.</p>
<pre class="r fold-show"><code>mnist_model %&gt;%
  fit(
    x = mnist_train$x, y = mnist_train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 1
  )</code></pre>
<p>Once our model is formed we can test it our on our testing data we withheld at the very beginning:</p>
<pre class="r fold-show"><code>mnist_predictions &lt;- predict(mnist_model, mnist_test$x)</code></pre>
<pre class="r fold-show"><code>mnist_model %&gt;% 
  evaluate(mnist_test$x, mnist_test$y, verbose = 0)</code></pre>
<pre><code>## $loss
## [1] 0.08473677
## 
## $acc
## [1] 0.9733</code></pre>
<p>To hit around 97% accuracy with little effort is pretty impressive. The MNIST data set is pretty boring, however, so lets try something harder.</p>
</div>
<div id="fashion-mnist" class="section level1">
<h1>Fashion MNIST</h1>
<p><a href="https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/">Fashion-MNIST</a> is a dataset of images consisting of a training set of 60,000 images and a test set of 10,000 images. Each image is a 28×28 grayscale image, associated with a label from 10 different categories.</p>
<pre class="r fold-show"><code>fashion &lt;- dataset_fashion_mnist()</code></pre>
<p>Taking a look at an example of the images:</p>
<pre class="r fold-show"><code># Plotting helper; fashion is global var
fashionplot &lt;- function(picked){
  fashion$train$x[picked,,] %&gt;% 
  # Wrangle
  as_tibble() %&gt;% 
  rowid_to_column(&quot;Y&quot;) %&gt;% 
  pivot_longer(-Y,names_to = &quot;X&quot;, values_to = &quot;PIX&quot;) %&gt;% 
  mutate(X = as.numeric(gsub(&quot;V&quot;,&quot;&quot;,X))) %&gt;%
  mutate(Y = abs(Y - max(Y)+1)) %&gt;%
  # Graph
  ggplot(aes(X,Y, fill = PIX)) + 
  geom_tile() + 
  scale_fill_gradient(low=&quot;white&quot;, high=&quot;black&quot;, guide = FALSE) +
  ggtitle(paste(&quot;Clothing Category:&quot;,fashion$train$y[picked])) +
  theme_void()
}
# Grid graph
pickcount &lt;- 9
fashionex &lt;- lapply(1:pickcount, fashionplot)
fashionexplot &lt;- arrangeGrob(grobs = fashionex,
                           ncol = 3,
                           nrow = 3)
plot(fashionexplot)</code></pre>
<p><img src="/post/2020-04-14-image-classification-with-mnist-and-mnist-like-data/index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r fold-show"><code>fashion_test = fashion$test
fashion_train = fashion$train
# Normalization; Speeds training time
fashion_train$x &lt;- fashion_train$x/255
fashion_test$x  &lt;- fashion_test$x/255</code></pre>
<pre class="r fold-show"><code>fashion_model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(28,28)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)</code></pre>
<pre class="r fold-show"><code>fashion_model %&gt;% 
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )</code></pre>
<pre class="r fold-show"><code>fashion_model %&gt;%
  fit(
    x = fashion_train$x, y = fashion_train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 1
  )</code></pre>
<pre class="r fold-show"><code>fashion_predictions &lt;- predict(fashion_model, fashion_test$x)</code></pre>
<pre class="r fold-show"><code>fashion_model %&gt;% 
  evaluate(fashion_test$x, fashion_test$y, verbose = 0)</code></pre>
<pre><code>## $loss
## [1] 0.371763
## 
## $acc
## [1] 0.8702</code></pre>
<p>Its worth pointing out that handling the FASHION MNIST dataset required nearly no difference in setup that the MNIST dataset, and we still also achieved 85%+ prediction accuracy.</p>
</div>
<div id="cifar-10" class="section level1">
<h1>CIFAR-10</h1>
<p>The <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a> consists of 60000 32x32 color images in 10 categories, with 6000 images for each category.</p>
<pre class="r fold-show"><code>cifar10 &lt;- dataset_cifar10()</code></pre>
<p>Taking a look at CIFAR-10 examples is a little trickier as it is a three-dimensional array. For this we will just utilize the <code>imager</code> library:</p>
<pre class="r fold-show"><code>par(mfrow = c(2,2))
for(i in 1:4){
  plot(cifar10$train$x[i,,,] %&gt;%
         as.cimg %&gt;%
         imrotate(,angle = 90),
       main = cifar10$train$y[i],
       axes = FALSE)
}</code></pre>
<p><img src="/post/2020-04-14-image-classification-with-mnist-and-mnist-like-data/index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r fold-show"><code>cifar10_test  &lt;- cifar10$test
cifar10_train &lt;- cifar10$train

cifar10_train$x &lt;- cifar10_train$x/255
cifar10_test$x  &lt;- cifar10_test$x/255</code></pre>
<p>As a proof of comparison… what if we treated this just like MNIST and FASHION MNIST and even tried dropping the color dimension?</p>
<pre class="r fold-show"><code>cifar10_bw_train &lt;- cifar10_train
cifar10_bw_test  &lt;- cifar10_test
cifar10_bw_test$x  &lt;- cifar10_bw_test$x[,,,1]
cifar10_bw_train$x &lt;- cifar10_bw_train$x[,,,1]</code></pre>
<pre class="r fold-show"><code>par(mfrow = c(2,2))
for(i in 1:4){
  plot(cifar10_bw_train$x[i,,] %&gt;%
         as.cimg %&gt;%
         imrotate(,angle = 90),
       main = cifar10_bw_train$y[i],
       axes = FALSE)
}</code></pre>
<p><img src="/post/2020-04-14-image-classification-with-mnist-and-mnist-like-data/index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r fold-show"><code>cifar10_bw_model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(32,32)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)

cifar10_bw_model %&gt;% 
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )

cifar10_bw_model %&gt;%
  fit(
    x = cifar10_bw_train$x, y = cifar10_bw_train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 1
  )

cifar10_bw_predictions &lt;- predict(cifar10_bw_model, cifar10_bw_test$x)

cifar10_bw_model %&gt;% 
  evaluate(cifar10_bw_test$x, cifar10_bw_test$y, verbose = 0)</code></pre>
<pre><code>## $loss
## [1] 1.957393
## 
## $acc
## [1] 0.2944</code></pre>
<pre class="r fold-show"><code>cifar10_model &lt;- keras_model_sequential() %&gt;%
  layer_flatten(input_shape = c(32,32,3)) %&gt;%
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_dense(10, activation = &quot;softmax&quot;)

cifar10_model %&gt;% 
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )

cifar10_model %&gt;%
  fit(
    x = cifar10_train$x, y = cifar10_train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 1
  )

cifar10_predictions &lt;- predict(cifar10_model, cifar10_test$x)

cifar10_model %&gt;% 
  evaluate(cifar10_test$x, cifar10_test$y, verbose = 0)</code></pre>
<pre><code>## $loss
## [1] 1.78544
## 
## $acc
## [1] 0.3588</code></pre>
<p>Unsurprisingly a more involved model is needed for CIFAR10. Training such models are a little more intensive than I want to host on my current hardware, so this is as far as I intend to take this post. It is interesting that including color wasn’t as drastic of an effect. Regardless hitting 30%+ in seconds of training is still noteworthy considering the CIFAR10 images are much less uniform in characteristics.</p>
</div>
