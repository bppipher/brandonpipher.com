---
title: Handling Github files in R
author: Brandon P. Pipher
date: '2020-03-25'
thumbnail: "/post/2020-03-25-handling-github-files-in-r_files/IMG_4427.jpg"
slug: handling-github-files-in-r
categories:
  - Data Analysis
tags:
  - Github
  - R
  - Dplyr
  - Readr
  - Stringr
subtitle: 'An example utilizing COVID-19 data from John Hopkins'
summary: 'An example utilizing COVID-19 data from John Hopkins'
image_preview: "/post/2020-03-25-handling-github-files-in-r_files/IMG_4427.jpg"
authors: []
lastmod: '2020-03-25T12:50:13-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
![](/post/2020-03-25-handling-github-files-in-r_files/IMG_4427.jpg)

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)
```

# John Hopkins COVID-19 Data
Given some recent updates to the way John Hopkins is adjusting their data formatting regarding COVID-19 ([read more here](https://github.com/CSSEGISandData/COVID-19/issues/1250)) the method of reading updated data has gotten easier.  This also makes utilizing something like the [coronavirus: The 2019 Novel Coronavirus COVID-19 (2019-nCoV) Dataset](coronavirus: The 2019 Novel Coronavirus COVID-19 (2019-nCoV) Dataset) for accessing up-to-date data more attractive, as you'll be gaining much of the same results albeit without the time series John Hopkins is still making available.

When the Daily Reports were being reported it required some management to maintain up-to-date access to the data.  I found figuring this out to be something that may come in handy for future endeavors and so I hope to detail the steps here.

This convenience update did come at a cost, however, as recovered cases are no longer being tracked.  However, as of this post they are still being tracked by Dailies meaning you could utilize the methods below to access this data as they seem committed to keeping old formats archived.

# The Process to handle files from a Github repo

The steps are fairly simple and utilize three packages:
```{r}
library(dplyr) # Handles Grammar of Data Manipulation
library(readr) # Handles importing excel file formats into R
library(stringr) # Handles management of file names from github
```
The first few steps are your basic file management.  We want to define our working directory, the URL the repo will download from, and the directory the data will be unpackaged at.
```{r, eval = FALSE}
# Assigning my directory
setwd("~/GitHub/brandonpipher.com/content/post")
```

```{r}
# Repo URL, found by copying link from "Clone or Download"
githubloc <-
  "https://github.com/CSSEGISandData/COVID-19/archive/master.zip"
# Where the repo will be downloaded and unpackaged at in the working directory
datadir <- "COVID-19"
destfile <- paste(datadir,"master.zip",sep = "/")
```
Adjust the above according to your own workflow.

Downloading the file and unpackaging it is also straightforward.
```{r, eval = FALSE}
# Downloading repo to destination
download.file(url = githubloc, destfile = destfile)
# Unzipping downloaded rip to destination of the 'data' folder
unzip(zipfile = destfile, exdir = datadir)
```
Something worth considering is a check to ensure you aren't re-downloading more than necessary.  

In a reactive environment a function such as the following might be handy, where we can calculate the time passed since the previous download. 
```{r, eval = FALSE}
minutesSince.DL = function(fileName) {
  (as.numeric(as.POSIXlt(Sys.time())) -  
   as.numeric(file.info(fileName)$ctime)) / 60
}
```
From here most of the work regarding inital data update has been completed.  The next step is accessing the data into R.
```{r}
# Manually adding the folder names from the repo to determine the data location
covid.dailies.loc <-
  paste(
    datadir,
    "COVID-19-master",
    "csse_covid_19_data/csse_covid_19_daily_reports",
    sep = "/"
  )
covid.dailies.loc
# Creating a list of available files, but ignoring the README
covid.dailies <-
  list.files(covid.dailies.loc)[!grepl("README", list.files(covid.dailies.loc))]
covid.dailies
```

And from here we've gained access to all of the files in a method that remains up-to-date as additions are made.  If we wanted to start to access these files by specifically tracking the range of available dates we can just use stringr to gain the vector of dates:
```{r}
covid.dailies.dates <- covid.dailies %>% str_remove(".csv")
covid.dailies.dates
```

From here utilizing `covid.dailies.dates` we can access the data according to available dates.   As an example:

```{r}
# Pulling an example of the most recent dataset
example.input <- covid.dailies.dates[length(covid.dailies.dates)]
example.input
# The example dataset
example.data <- read_csv(paste(covid.dailies.loc, paste0(example.input,'.csv'), sep = "/"))
```

And from here we could imagine `example.input` as being assigned reactively, and so we've created a way of managing the data from Github (or any other public URL, really) in real-time.

```{r}
example.data %>% 
  group_by(`Country_Region`) %>% 
  summarize(Total = sum(Confirmed)) %>%
  arrange(desc(Total))
```

# How to access the data Post-Update

Coming back to the present, remember there were updates made simplifying this process.    No longer would anyone need to access the files from the dailies and manually wrangle the data.  

Instead one just goes to the [Github Repo](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) and access is as easy as pulling the files:

`time_series_covid19_confirmed_global.csv`
`time_series_covid19_deaths_global.csv`

Or, if you want to do it straight from a package hosted on CRAN and aren't concerned about time-series data:
```{r}
library(coronavirus)
head(coronavirus)
```
More info about the `coronavirus` package can be found [here](https://github.com/RamiKrispin/coronavirus).
